{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c6fcbd",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction System â€” Final (Clean & Pipeline-ready)\n",
    "**Purpose:** polished, well-commented notebook that builds a reproducible pipeline (preprocessing + model) and saves artifacts for deployment.\n",
    "\n",
    "**What changed:**\n",
    "- Replaced separate scaler + manual encoding with a `ColumnTransformer` (StandardScaler + OneHotEncoder)\n",
    "- Saved a full `Pipeline` (preprocessing + model) so the Streamlit app can load and predict reliably\n",
    "- Saved helper artifacts: `feature_cols`, `num_cols`, `cat_cols`, and OHE feature names\n",
    "- Added clear comments and sections for readability and for recruiters to inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0ca88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done.\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling & preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 42\n",
    "print('Imports done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b148c85",
   "metadata": {},
   "source": [
    "## 2) Load datasets\n",
    "We load the two CSVs you used in the original notebook: the 80% train split and 20% test split. Make sure the `data/` folder contains these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc50146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2666, 20)\n",
      "Test shape:  (667, 20)\n"
     ]
    }
   ],
   "source": [
    "# Paths - adjust if your repo is structured differently\n",
    "DATA_DIR = Path('data')\n",
    "train_path = \"C:/Users/abhis/Downloads/churn-bigml-80.csv\"\n",
    "test_path = \"C:/Users/abhis/Downloads/churn-bigml-20.csv\"\n",
    "\n",
    "# Read CSVs\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\") \n",
    "print(f\"Test shape:  {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184e7c7",
   "metadata": {},
   "source": [
    "## 3) Quick EDA & target check\n",
    "Check target distribution and a few sample rows. This helps understand class balance and feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39520d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target value counts (train):\n",
      "Churn\n",
      "False    2278\n",
      "True      388\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Account length</th>\n",
       "      <th>Area code</th>\n",
       "      <th>International plan</th>\n",
       "      <th>Voice mail plan</th>\n",
       "      <th>Number vmail messages</th>\n",
       "      <th>Total day minutes</th>\n",
       "      <th>Total day calls</th>\n",
       "      <th>Total day charge</th>\n",
       "      <th>Total eve minutes</th>\n",
       "      <th>Total eve calls</th>\n",
       "      <th>Total eve charge</th>\n",
       "      <th>Total night minutes</th>\n",
       "      <th>Total night calls</th>\n",
       "      <th>Total night charge</th>\n",
       "      <th>Total intl minutes</th>\n",
       "      <th>Total intl calls</th>\n",
       "      <th>Total intl charge</th>\n",
       "      <th>Customer service calls</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>84</td>\n",
       "      <td>408</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>299.4</td>\n",
       "      <td>71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>61.9</td>\n",
       "      <td>88</td>\n",
       "      <td>5.26</td>\n",
       "      <td>196.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.86</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK</td>\n",
       "      <td>75</td>\n",
       "      <td>415</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>113</td>\n",
       "      <td>28.34</td>\n",
       "      <td>148.3</td>\n",
       "      <td>122</td>\n",
       "      <td>12.61</td>\n",
       "      <td>186.9</td>\n",
       "      <td>121</td>\n",
       "      <td>8.41</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State  Account length  Area code International plan Voice mail plan  \\\n",
       "0    KS             128        415                 No             Yes   \n",
       "1    OH             107        415                 No             Yes   \n",
       "2    NJ             137        415                 No              No   \n",
       "3    OH              84        408                Yes              No   \n",
       "4    OK              75        415                Yes              No   \n",
       "\n",
       "   Number vmail messages  Total day minutes  Total day calls  \\\n",
       "0                     25              265.1              110   \n",
       "1                     26              161.6              123   \n",
       "2                      0              243.4              114   \n",
       "3                      0              299.4               71   \n",
       "4                      0              166.7              113   \n",
       "\n",
       "   Total day charge  Total eve minutes  Total eve calls  Total eve charge  \\\n",
       "0             45.07              197.4               99             16.78   \n",
       "1             27.47              195.5              103             16.62   \n",
       "2             41.38              121.2              110             10.30   \n",
       "3             50.90               61.9               88              5.26   \n",
       "4             28.34              148.3              122             12.61   \n",
       "\n",
       "   Total night minutes  Total night calls  Total night charge  \\\n",
       "0                244.7                 91               11.01   \n",
       "1                254.4                103               11.45   \n",
       "2                162.6                104                7.32   \n",
       "3                196.9                 89                8.86   \n",
       "4                186.9                121                8.41   \n",
       "\n",
       "   Total intl minutes  Total intl calls  Total intl charge  \\\n",
       "0                10.0                 3               2.70   \n",
       "1                13.7                 3               3.70   \n",
       "2                12.2                 5               3.29   \n",
       "3                 6.6                 7               1.78   \n",
       "4                10.1                 3               2.73   \n",
       "\n",
       "   Customer service calls  Churn  \n",
       "0                       1  False  \n",
       "1                       1  False  \n",
       "2                       0  False  \n",
       "3                       2  False  \n",
       "4                       3  False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target check\n",
    "print('Target value counts (train):')\n",
    "print(df_train['Churn'].value_counts(dropna=False))\n",
    "print('\\nSample rows:')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35352c",
   "metadata": {},
   "source": [
    "## 4) Preprocessing plan\n",
    "We will:\n",
    "1. Drop identifier columns (phone numbers) that do not help prediction.\n",
    "2. Separate feature columns into numeric and categorical.\n",
    "3. Build a ColumnTransformer that scales numeric features and one-hot-encodes categorical features.\n",
    "4. Create a Pipeline that includes preprocessing + RandomForest model.\n",
    "\n",
    "Saving the full pipeline avoids shape mismatches at prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f507ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Drop identifiers if present\n",
    "drop_candidates = ['phone', 'Phone', 'PhoneNumber']\n",
    "for c in drop_candidates:\n",
    "    if c in df_train.columns:\n",
    "        df_train = df_train.drop(columns=[c])\n",
    "        df_test = df_test.drop(columns=[c])\n",
    "        print(f\"Dropped identifier column: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa49478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 19\n",
      "Numerical columns (16): ['Account length', 'Area code', 'Number vmail messages', 'Total day minutes', 'Total day calls', 'Total day charge', 'Total eve minutes', 'Total eve calls', 'Total eve charge', 'Total night minutes', 'Total night calls', 'Total night charge', 'Total intl minutes', 'Total intl calls', 'Total intl charge', 'Customer service calls']\n",
      "Categorical columns (3): ['State', 'International plan', 'Voice mail plan']\n"
     ]
    }
   ],
   "source": [
    "# 4.2 Feature/target split\n",
    "TARGET = 'Churn'\n",
    "feature_cols = [c for c in df_train.columns if c != TARGET]\n",
    "print('Total features:', len(feature_cols))\n",
    "\n",
    "# Identify categorical vs numeric using dtype\n",
    "cat_cols = [c for c in feature_cols if df_train[c].dtype == 'object' or df_train[c].dtype.name == 'category']\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "print('Numerical columns ({}):'.format(len(num_cols)), num_cols)\n",
    "print('Categorical columns ({}):'.format(len(cat_cols)), cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f7bc1",
   "metadata": {},
   "source": [
    "## 5) Build ColumnTransformer + Pipeline\n",
    "We use `StandardScaler` for numeric columns and `OneHotEncoder(handle_unknown='ignore')` for categorical columns. The pipeline combines them and ends with a RandomForest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e145a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline created.\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Define transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# FIXED: use 'sparse_output=False' instead of 'sparse=False'\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 5.2 Define model and pipeline\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('model', rf)\n",
    "])\n",
    "\n",
    "print(\"Pipeline created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bd43e",
   "metadata": {},
   "source": [
    "## 6) Prepare training & testing matrices\n",
    "We will fit the pipeline on the train set (80% file) and use the 20% file as a final holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2732ba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes => X_train: (2666, 19) X_test: (667, 19)\n"
     ]
    }
   ],
   "source": [
    "# Make sure target is consistent dtype\n",
    "y_train = df_train[TARGET].astype(str)\n",
    "X_train = df_train[feature_cols].copy()\n",
    "\n",
    "y_test = df_test[TARGET].astype(str)\n",
    "X_test = df_test[feature_cols].copy()\n",
    "\n",
    "print('Shapes => X_train:', X_train.shape, 'X_test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afded2",
   "metadata": {},
   "source": [
    "## 7) Hyperparameter tuning with RandomizedSearchCV\n",
    "We tune Random Forest parameters using RandomizedSearchCV. Note: the pipeline includes preprocessing so the CV respects train-time preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7810f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best params: {'model__n_estimators': 100, 'model__min_samples_split': 2, 'model__max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 5, 10],\n",
    "    'model__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rsearch = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=8, cv=3, scoring='f1', n_jobs=-1, random_state=RANDOM_STATE, verbose=1)\n",
    "rsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Best params:', rsearch.best_params_)\n",
    "best_pipeline = rsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa52b2",
   "metadata": {},
   "source": [
    "## 8) Evaluation on holdout test set\n",
    "Evaluate final pipeline on the 20% holdout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9067735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      1.00      0.97       572\n",
      "        True       0.98      0.62      0.76        95\n",
      "\n",
      "    accuracy                           0.94       667\n",
      "   macro avg       0.96      0.81      0.86       667\n",
      "weighted avg       0.95      0.94      0.94       667\n",
      "\n",
      "Confusion Matrix:\n",
      " [[571   1]\n",
      " [ 36  59]]\n",
      "ROC AUC: nan\n",
      "Average precision (PR AUC): 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Predict & report\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "# For probability, pipeline.predict_proba returns two columns; positive class probability depends on label ordering\n",
    "y_proba = best_pipeline.predict_proba(X_test)\n",
    "# If 'Yes' corresponds to class index 1 (common), take column 1. We'll be cautious.\n",
    "try:\n",
    "    classes = best_pipeline.named_steps['model'].classes_\n",
    "except Exception:\n",
    "    classes = None\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "\n",
    "# Compute ROC/PR using binary encoding (Yes->1, else 0)\n",
    "y_test_bin = (y_test == 'Yes').astype(int)\n",
    "if y_proba.shape[1] == 2:\n",
    "    y_score = y_proba[:, 1]\n",
    "else:\n",
    "    # fallback: take first column\n",
    "    y_score = y_proba[:, 0]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_bin, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "avg_prec = average_precision_score(y_test_bin, y_score)\n",
    "print(f'Average precision (PR AUC): {avg_prec:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e671b8",
   "metadata": {},
   "source": [
    "## 9) Save pipeline and helper artifacts\n",
    "Save everything required by the Streamlit app: full pipeline + column metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9354b0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pipeline to models\\churn_pipeline.pkl\n",
      "Saved feature metadata and encoders. Files in models/: [WindowsPath('models/cat_cols.pkl'), WindowsPath('models/churn_pipeline.pkl'), WindowsPath('models/feature_cols.pkl'), WindowsPath('models/feature_names_after_ohe.pkl'), WindowsPath('models/num_cols.pkl')]\n"
     ]
    }
   ],
   "source": [
    "models_dir = Path('models')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save full pipeline (preprocessing + model)\n",
    "pipeline_path = models_dir / 'churn_pipeline.pkl'\n",
    "joblib.dump(best_pipeline, pipeline_path)\n",
    "print('Saved pipeline to', pipeline_path)\n",
    "\n",
    "# Save metadata and helper artifacts\n",
    "joblib.dump(feature_cols, models_dir / 'feature_cols.pkl')\n",
    "joblib.dump(num_cols, models_dir / 'num_cols.pkl')\n",
    "joblib.dump(cat_cols, models_dir / 'cat_cols.pkl')\n",
    "\n",
    "# Save the OneHotEncoder feature names (after fitting)\n",
    "ohe = best_pipeline.named_steps['preproc'].named_transformers_['cat']\n",
    "ohe_feature_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "feature_names_after_ohe = list(num_cols) + ohe_feature_names\n",
    "joblib.dump(feature_names_after_ohe, models_dir / 'feature_names_after_ohe.pkl')\n",
    "\n",
    "print('Saved feature metadata and encoders. Files in models/:', list(models_dir.iterdir()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
